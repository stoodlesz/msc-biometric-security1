ðŸ“… Dev Log â€” 16 September 2025

Goal: Finalise dataset setup, train baseline model, and prepare Jupyter notebook structure for main dissertation project.

Achievements

1. Dataset Setup

Created Datasets/ directory with structured subfolders for:

lfw-dataset/lfw-deepfunneled

celeba-dataset/

Updated download_datasets.sh to:

Use working Kaggle download commands

Unzip directly into usable folders

Updated README.md with clear instructions for dataset setup and Kaggle API usage.

2. Jupyter Notebook Structure Finalised
Created 3 separate notebooks to logically split the project:

1_data_loading_preprocessing.ipynb
â€“ Loads and preprocesses datasets (LFW and CelebA), applies transforms, inspects class distribution.

2_model_training_fgsm_attack.ipynb
â€“ Loads pretrained ResNet18, modifies classifier for LFW classes, trains on a subset, then runs FGSM attack and logs results.

3_defence_and_evaluation.ipynb
â€“ Placeholder setup for testing defences (e.g., JPEG compression, detection methods) and evaluating accuracy, latency, ASR, etc.

3. Version Control Fixes

Resolved broken pushes by:

Removing unnecessary large files (e.g., model checkpoints)

Clearing output from notebooks with nbconvert

Updating .gitignore to exclude datasets and .pt files

Lessons & Considerations

Adversarial training and model evaluation require good classification accuracy â€” current losses are high; further training or simplified class subset may be needed.

LFW has 5,749 classes â€” training with full set is infeasible on CPU. Consider reducing dataset to top N classes.

Using pretrained ResNet with transfer learning is more aligned with SME scenarios than building from scratch.

Keeping notebooks small and modular improves clarity and reproducibility.

Next Steps

Improve training loop and optionally freeze lower ResNet layers

Begin implementing JPEG preprocessing defence

Write evaluation functions for FAR, FRR, ASR, and latency

Begin drafting dissertation Methodology section