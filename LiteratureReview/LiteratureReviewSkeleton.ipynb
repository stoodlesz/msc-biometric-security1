{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f65c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Attacks on AI-Based Biometric Authentication Systems: Detection and Defence in Practice\n",
    "\n",
    "## 1. Introduction and Scope\n",
    "\n",
    "Biometric authentication (e.g. facial recognition, fingerprint scanning, voiceprint) is becoming a standard feature in AI-based access systems across consumer, enterprise, and governmental platforms. While it offers convenience and perceived security, these systems are increasingly vulnerable to adversarial machine learning attacks, such as carefully crafted inputs that trigger false acceptances or rejections.\n",
    "\n",
    "This project focuses specifically on **facial recognition** due to the availability of pretrained models, accessible open datasets, and its high deployment rate across real-world applications. It also enables clear visualisation of adversarial perturbations.\n",
    "\n",
    "However, deploying robust defences remains a challenge, particularly for small-to-medium-sized enterprises (SMEs) with limited technical resources. Most defences are either computationally expensive or lack reproducibility and transparency.\n",
    "\n",
    "### Project Aims\n",
    "\n",
    "- Develop a reproducible FaceNet-based authentication prototype  \n",
    "- Simulate adversarial attacks (e.g., FGSM, PGD)  \n",
    "- Implement lightweight, explainable defences (e.g., JPEG preprocessing, anomaly detection)  \n",
    "- Measure practical effectiveness using metrics like FAR, FRR, ASR, and latency  \n",
    "- Produce a toolkit or checklist usable by non-experts for secure ML system deployment  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Facial Recognition and AI Architecture\n",
    "\n",
    "- Brief overview of biometric modalities\n",
    "- Focus on convolutional neural network (CNN) based face recognition (FaceNet, ResNet, ArcFace)\n",
    "- Dataset choices and justification: LFW, CelebA, CASIA-WebFace\n",
    "- Emphasis on reproducibility, public availability, and research relevance\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Adversarial Attacks on Biometric Systems\n",
    "\n",
    "- Key methods: FGSM, PGD, Carlini-Wagner\n",
    "- White-box vs black-box attacks\n",
    "- Real-world examples (e.g. adversarial glasses, spoofed videos)\n",
    "- Summary of how such attacks degrade reliability of biometric systems\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Defence Strategies and Model Hardening\n",
    "\n",
    "- Techniques: adversarial training, input preprocessing, anomaly detection\n",
    "- Strengths and limitations of each method\n",
    "- Practicality and applicability for SME deployment\n",
    "- Emphasis on low-barrier implementation\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Measuring Effectiveness and Usability\n",
    "\n",
    "- Metrics: FAR (False Acceptance Rate), FRR (False Rejection Rate), ASR (Attack Success Rate), and latency\n",
    "- Simulated usability insights using system performance (no human studies)\n",
    "- Highlight trade-offs between usability and robustness in resource-constrained environments\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Ethical and Regulatory Considerations\n",
    "\n",
    "- GDPR classification of biometric data (special category)\n",
    "- Concepts: revocability, unlinkability, irreversibility\n",
    "- Requirements for transparency, consent, fairness in deployment\n",
    "- Regulatory references (GDPR, ISO/IEC 27001)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Secure Engineering and Research Gap\n",
    "\n",
    "While adversarial attacks are well studied in academia, practical defences are often inaccessible to smaller organisations. Current issues include:\n",
    "\n",
    "- High computational cost of training robust models\n",
    "- Poor reproducibility across different datasets and model types\n",
    "- Lack of lightweight tooling or evaluation metrics for deployment\n",
    "\n",
    "This project addresses these issues by:\n",
    "- Delivering a working prototype\n",
    "- Implementing and testing attacks and defences\n",
    "- Offering a practical methodology for secure engineering of AI-authentication systems\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary and Research Justification\n",
    "\n",
    "- Vulnerabilities in AI biometrics are well documented\n",
    "- Real-world defences are incomplete, difficult to implement, or poorly documented\n",
    "- SMEs and non-experts need easier ways to validate and defend systems\n",
    "- This project aims to deliver:\n",
    "  - A reproducible pipeline\n",
    "  - Explainable methods\n",
    "  - A lightweight validation toolkit for SMEs and developers\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š References\n",
    "\n",
    "Abdullahi, S.M., 2024. Biometric template attacks and recent protection mechanisms. *Patterns*, 5(4), p.100500.  \n",
    "Alrawili, R., Alshehri, S., Alesawe, A. and Alsubhi, K., 2023. Biometric user authentication: A survey. *arXiv preprint*, arXiv:2311.13416.  \n",
    "Dong, X., Park, J., Jin, Z., Teoh, A.B.J., Tistarelli, M. and Wong, K.S., 2019. On the risk of cancelable biometrics. *arXiv preprint*, arXiv:1910.07770.  \n",
    "Goodfellow, I.J., Shlens, J. and Szegedy, C., 2015. Explaining and harnessing adversarial examples. *arXiv preprint*, arXiv:1412.6572.  \n",
    "Information Commissionerâ€™s Office (ICO), 2025. *Biometric template protection: revocability and renewability in biometric systems*. Available from: https://ico.org.uk [Accessed 3 September 2025].  \n",
    "International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC), 2022. *ISO/IEC 27001:2022 â€” Information Security, Cybersecurity and Privacy Protection*.  \n",
    "Kilany, S. and Mahfouz, A., 2025. A survey of deep face verification attacks and defences. *Scientific Reports*, 15(1), p.30861.  \n",
    "Liu, Z., Luo, P., Wang, X. and Tang, X., 2015. Deep learning face attributes in the wild. *Proceedings of ICCV*, pp.3730â€“3738.  \n",
    "Madry, A., Makelov, A., Schmidt, L., Tsipras, D. and Vladu, A., 2018. Towards deep learning models resistant to adversarial attacks. *ICLR*.  \n",
    "Massoli, F.V., 2021. Detection of face recognition adversarial attacks. *Applied Sciences*, 11(4), p.1587.  \n",
    "Mecke, L., Saad, A., Prange, S., Gruenefeld, U., Schneegass, S. and Alt, F., 2024. Assessing perception and usage of biometrics. *arXiv preprint*, arXiv:2410.12661.  \n",
    "Moosavi-Dezfooli, S.M., Fawzi, A. and Frossard, P., 2016. DeepFool: A simple and accurate method to fool deep neural networks. *CVPR*, pp.2574â€“2582.  \n",
    "National Institute of Standards and Technology (NIST), 2022. Face Recognition Vendor Test (FRVT). Available from: https://www.nist.gov [Accessed 1 September 2025].  \n",
    "OWASP Foundation, 2023. Machine Learning Security Top 10. Available from: https://owasp.org [Accessed 1 September 2025].  \n",
    "Schroff, F., Kalenichenko, D. and Philbin, J., 2015. FaceNet: A unified embedding for face recognition and clustering. *CVPR*, pp.815â€“823.  \n",
    "Sharif, M., Bhagavatula, S., Bauer, L. and Reiter, M.K., 2016. Accessorize to a crime. *ACM CCS*, pp.1528â€“1540.  \n",
    "Stegman, J., 2023. Quantifying security of behavioural biometrics. MSc Thesis. University of Guelph.  \n",
    "Vakhshiteh, F., Nickabadi, A. and Ramachandra, R., 2020. Adversarial attacks against face recognition. *arXiv preprint*, arXiv:2007.11709.  \n",
    "Wachter, S. and Mittelstadt, B., 2019. A right to reasonable inferences. *Columbia Business Law Review*, 2019(2), pp.494â€“620.  \n",
    "Wang, B., Yao, Y., Chen, H., Xie, B., Wang, H. and Li, B., 2020. Mitigating adversarial effects through randomised preprocessing. *NeurIPS Workshop*.  \n",
    "Yampolskiy, R.V., 2020. Secure and explainable AI. *Journal of Cybersecurity and Privacy*, 1(1), pp.39â€“59.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
