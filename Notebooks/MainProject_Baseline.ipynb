{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b8f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5749\n",
      "Using device: cpu\n",
      "Batch 1/20 - Loss: 8.5900\n",
      "Batch 2/20 - Loss: 8.7848\n",
      "Batch 3/20 - Loss: 8.5426\n",
      "Batch 4/20 - Loss: 8.8416\n",
      "Batch 5/20 - Loss: 8.8625\n",
      "Batch 6/20 - Loss: 8.3139\n",
      "Batch 7/20 - Loss: 8.8097\n",
      "Batch 8/20 - Loss: 8.9172\n",
      "Batch 9/20 - Loss: 8.8381\n",
      "Batch 10/20 - Loss: 9.3917\n",
      "Batch 11/20 - Loss: 9.3120\n",
      "Batch 12/20 - Loss: 9.2534\n",
      "Batch 13/20 - Loss: 10.6001\n",
      "Batch 14/20 - Loss: 10.1084\n",
      "Batch 15/20 - Loss: 10.6524\n",
      "Batch 16/20 - Loss: 10.1895\n",
      "Batch 17/20 - Loss: 11.4078\n",
      "Batch 18/20 - Loss: 9.7995\n",
      "Batch 19/20 - Loss: 10.1983\n",
      "Batch 20/20 - Loss: 10.3403\n",
      "\n",
      "Finished training 20 batches in 8.90 seconds.\n",
      "✅ Found correctly classified image at attempt 20\n",
      "Original prediction: George_W_Bush\n",
      "Adversarial prediction: Donald_Rumsfeld\n",
      "Clean confidence: 0.0530\n",
      "Adversarial confidence: 0.0317\n"
     ]
    }
   ],
   "source": [
    "# MSc Project: Biometric Security - Baseline Model\n",
    "## Author: Stella Williams\n",
    "## Date: 04.10.2025\n",
    "\n",
    "#---\n",
    "\n",
    "## 1. Imports and Setup\n",
    "## 2. Dataset Loading (LFW)\n",
    "## 3. Load Pretrained ResNet18 and Freeze Base\n",
    "## 4. Add Custom Classification Head\n",
    "## 5. Train for a Few Epochs\n",
    "## 6. Save the Model\n",
    "## 7. Evaluate on Clean Samples\n",
    "## 8. Set Up for FGSM and Defence Experiments (Future Work)\n",
    "\n",
    "# 1. Imports and Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 2. Dataset Loading\n",
    "\n",
    "# Adjust path if needed\n",
    "lfw_path = \"../Datasets/lfw-dataset\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # for ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "lfw_dataset = datasets.ImageFolder(root=lfw_path, transform=transform)\n",
    "lfw_loader = DataLoader(lfw_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "# Check class count\n",
    "num_classes = len(lfw_dataset.classes)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Resnet stuff for classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze early layers (optional)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final classification layer to match LFW classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(lfw_dataset.classes))\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set Up Loss and Optimiser\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define loss and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Limit training for prototyping\n",
    "num_batches = 20\n",
    "start_time = time.time()\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(lfw_loader):\n",
    "    if batch_idx >= num_batches:\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Batch {batch_idx+1}/{num_batches} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinished training {num_batches} batches in {(time.time() - start_time):.2f} seconds.\")\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet_lfw_prelim.pth\")\n",
    "\n",
    "# --- FGSM Attack Function ---\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# selecting image for ATTACKKKK \n",
    "model.eval()\n",
    "attempts = 0\n",
    "max_attempts = 30  # Try 30 images max\n",
    "\n",
    "for images, labels in lfw_loader:\n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i].unsqueeze(0)\n",
    "        label = torch.tensor([labels[i]])\n",
    "\n",
    "        image.requires_grad = True\n",
    "        output = model(image)\n",
    "        init_pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "        if init_pred.item() == label.item():\n",
    "            print(f\"✅ Found correctly classified image at attempt {attempts+1}\")\n",
    "            loss = criterion(output, label)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            epsilon = 0.1\n",
    "            data_grad = image.grad.data\n",
    "            perturbed_image = fgsm_attack(image, epsilon, data_grad)\n",
    "\n",
    "            output_adv = model(perturbed_image)\n",
    "            adv_pred = output_adv.max(1, keepdim=True)[1]\n",
    "\n",
    "            print(f\"Original prediction: {lfw_dataset.classes[init_pred.item()]}\")\n",
    "            print(f\"Adversarial prediction: {lfw_dataset.classes[adv_pred.item()]}\")\n",
    "            print(f\"Clean confidence: {torch.softmax(output, dim=1)[0][init_pred].item():.4f}\")\n",
    "            print(f\"Adversarial confidence: {torch.softmax(output_adv, dim=1)[0][adv_pred].item():.4f}\")\n",
    "\n",
    "            # Visualise\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torchvision.transforms.functional as TF\n",
    "\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Original\")\n",
    "            plt.imshow(TF.to_pil_image(image.squeeze().detach()))\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"Adversarial\")\n",
    "            plt.imshow(TF.to_pil_image(perturbed_image.squeeze().detach()))\n",
    "            plt.axis('off')\n",
    "            plt.suptitle(f\"Epsilon: {epsilon}\")\n",
    "            plt.show()\n",
    "\n",
    "            break  # Stop after first success\n",
    "        attempts += 1\n",
    "        if attempts >= max_attempts:\n",
    "            print(\"❌ No correctly predicted image found after 30 tries.\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "    \n",
    "    # Run the same FGSM loop with different epsilon values:\n",
    "\n",
    "epsilons = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "\n",
    "# Plot how adversarial success rate changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be364d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4074c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba67d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47aa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50986d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119efed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196549e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
