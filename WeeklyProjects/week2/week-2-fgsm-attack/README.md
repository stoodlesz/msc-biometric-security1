# FGSM Adversarial Attack on LFW

This mini project demonstrates how the Fast Gradient Sign Method (FGSM) can be used to generate adversarial examples against facial recognition systems trained on the LFW dataset.

## Goals
- Load LFW dataset and simulate a simple face verification task
- Implement FGSM to craft adversarial examples
- Visualise perturbations and compare classification confidence
- Discuss implications for real-world authentication systems

## Next Steps
- Try stronger attacks (e.g., PGD, Carlini-Wagner)
- Analyse model robustness with defence mechanisms
